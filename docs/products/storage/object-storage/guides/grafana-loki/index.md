---
title: "Using Grafana Loki with Object Storage"
description: "Learn how to use the Loki aggregation and visualization system to store and analyze logs from Linode's Object Storage."
authors: ["Linode"]
---

Grafana Loki is a log aggregation and visualization system for modern, cloud-native environments. It offers a cost-effective, scalable solution for processing large volumes of log data generated by modern applications and microservices. With Grafana Loki, users can query and visualize logs from cloud-native workloads. Loki uses a label-based indexing system. This makes it an ideal choice for observability, monitoring, alerting and data analysis..

## Before you start

[Grafana Loki](https://grafana.com/docs/loki/latest/configure/) can seamlessly integrate with Linode Object Storage as a backend storage solution using the S3 protocol and S3 storage configuration. You can configure Loki to use Linode Object Storage to offload the storage and management of log data to a scalable and reliable cloud storage platform. This helps to efficiently store and analyze logs without worrying about infrastructure management overhead.

Before you apply the Best Practices to your configuration, review these basic concepts to understand the Loki caching and storage workflow. 

### Memcached Cache Store

- The best practices examples use Memcached, the most popular local caching system. Memcached serves as Loki's cache store to provide a fast, distributed caching layer.
- The Memcached configuration stores chunks of log data and associated metadata in its key-value store.

### Chunk Indexing and Caching

- Logs ingested into Loki are typically grouped into chunks. These chunks represent a time-bound segment of log data. The structure of the chunks allows efficient querying based on time ranges and labels.
- The Loki ingester indexes and caches chunks in Memcached for rapid retrieval.
- When a Loki query occurs, the query engine first checks Memcached for the required chunks.

### Caching During Writing

- During ingestion, Memcached may cache chunks immediately after they are written to the backend storage such as, Linode Object Store.
- This proactive caching ensures that recently ingested log data is available for querying. It also avoids the latency that occurs when fetching it from the backend storage.
- Loki manages the indexes and chunks separately even though it may use the same backend storage for both. 

### Caching During Reading

- When a query occurs the query engine checks the Memcached for the required chunks.
- If the query finds the chunks in the cache, it retrieves the chunks directly from Memcached. This results in low-latency query responses.
- Loki fetches chunks that are not present in the cache or chunks evicted due to caching policies from the backend storage.

### Eviction and Management

- Memcached manages its own eviction policies. These policies use factors such as memory usage, access patterns, and expiration times.
- The cache may evict the least frequently accessed chunks or chunks that have exceeded their time-to-live (TTL). This makes room for new data.
- Loki's configuration may include parameters for tuning the eviction policies and cache size to optimize performance and resource utilization.


## Best Practices

Review these best practices for leveraging Linode Object Storage with Grafana Loki. These tips focus on Loki’s caching and storage configuration and provides specific recommendations for the integration. 

### Enable caching on Loki 

Caching helps to ensure that requests and throughput are not rate-limited. It can also promote optimal latency for typical enterprise workloads. The caching in Grafana Loki balances query performance, resource utilization, and data durability. By intelligently managing caching, eviction, and storage operations, Loki optimizes the trade-offs between responsiveness and scalability in log aggregation and visualization. Any caching configuration, or lack of it, has direct implications on the number of requests, such as GETs, going forward to Object Storage. Today Object Storage supports 750 mixed requests per second (RPS) per bucket. 

### Choose a multi-tenancy workload type with an independent storage config

You should understand your Loki workload type and evaluate whether it’s single or multi-tenant. Multi-tenancy is the default for Loki and is common for enterprise solutions. Please see the Loki documentation for information about multi-tenancy configurations. Each Loki tenant has a separate configuration per tenant that includes caching, such as Memcached, and storage, such as Linode Object Storage.

In addition to providing data isolation, this model allows horizontal scalability and storage load sharding per tenant. 
- A configuration with one tenant and one bucket only supports 750 mixed RPS for your entire workload. 
- An aggregated workload of 10K queries per second across all your grafana graphs with a cache configuration that yields 90% hit ratio, ~1K requests will land on the backend Object Storage bucket.
- With one tenant and one bucket, your workload will already exceed the rate limit for the bucket. 
- A configuration with two tenants and two buckets, one each for the org/division/workload type, gets twice the RPS and the workload is unlikely to be rate limited. 

### Configure the Loki cache 

Loki has comprehensive support for caching and has several tunables and configurable parameters. Review these recommended options to learn more.

#### Use an optimized cache store like Memcached

In-memory cache is auto-enabled in Loki. However it is recommended to use an optimized cache store like Memcached. To configure Memcached, refer to the [Grafana documentation](https://grafana.com/docs/loki/latest/operations/caching/). 

#### Configure the chunk_store_config block

The chunk_store_config block configures how chunks are cached and how long to wait before saving them to the backing store.

Some of the key config parameters include “max_chunk_age” and “chunk_idle_period”. These parameters determine how long the chunks are cached before being flushed out. 


Use “Default_validity”  for results caching and “chunk_target_size”.



To determine the right values for these parameters for your use-case, it’s critical to consider the following:
- **Load**: The amount of log data in bytes being produced per day by your tenant workload. For example, GBs/day. 
- **Log access patterns**: Determines whether the log data access patterns for your use-case skews towards recent data only such as, <12h old, or older data such as, > 4-5 days.
- **Cache capacity considerations**: Determines whether your Loki deployment has enough resources such as RAM, CPU, local disk space available and allocated for caching. 
- **Cost considerations**: Estimates costs for managing the cache locally for Loki, given the memory and disk space capacity requirements.

To learn more you can read the [Grafana Cloud blog post](https://grafana.com/blog/2023/08/23/how-we-scaled-grafana-cloud-logs-memcached-cluster-to-50tb-and-improved-reliability/) that discusses how appropriate cache sizing impacts performance and reliability.

## Loki Storage Configuration

The s3_storage_config block configures the connection to the Linode S3 Object Storage backend.

The “bucketnames” storage config parameter allows the Loki tenant workloads to specify more than one bucket to shard the chunks (logs data) across. It’s highly recommended that you configure more than one bucket, and possibly many depending on the load. This helps with scalability and load balancing since rate limits are enforced at bucket level.

The following storage backoff settings are also important.

These parameters determine how Loki manages the storage requests when Linode Object Storage enforces rate limits (say due to request rates higher than the allowed limits or any other reasons). If not configured properly, there can be a cascading effect where the retries contribute further to the request rates resulting in perpetual or longer than ideal limits enforcement. 

Given the Linode Object Storage rate-limiting implementation, the following values are highly recommended:
- min_period: 2s
- max_period: 5s
- max_retries: 5

#### Loki Chunk Size Configuration

The following chunk size and related config parameters are supported. 

Configure chunk sizes between 1.5 MB and 2 MB. 

The chunk_target_size directly translates to the object sizes in Linode Object storage and determines the:
- Number of overall PUT requests.
- Number of objects stored in the buckets.
- Effective bandwidth required for the Object Storage operations.


Review the following example to learn why it’s important  to choose the chunk_target_size value carefully. 

If your workload generates 5 TB of logging data per day and the chunk size is 1.5 MB, then on an average (even distribution) it will generate:
- ~38 PUT requests per second. 
- 57 MBps, for example 456 Mbps of throughput out to the storage.
- ~ 3.3 Mil objects per day.  

If the chunk size is 2 MB:
- ~29 PUT requests per second would be generated for the same aggregate throughput to the storage but ~2.5 Mil objects per day.  
- The number of requests and throughput for GET requests per second on the other hand are highly dependent on the window size/period of time for which logs are being pulled and whether they are in cache or not. 
- Assuming cache miss and ~500 GET requests per second with 2 MB chunk size would generate approximately ~1 GBps ie. 8 Gbps of throughput! 

### Maintain your Loki deployment

Here are some additional best practices for continued operational healthiness of your Loki deployment:
- Monitor cache utilization and query performance using Grafana dashboards and Prometheus metrics.
- Experiment with different cache configurations to find the optimal balance between memory usage and query responsiveness.
- Regularly review and adjust cache parameters based on changing workload characteristics and system resource availability.
- Periodically delete, and configure Linode Object Storage bucket lifecycle policies to delete objects, such as log data, that are no longer required for your user-case.

    ![Screenshot of the Enable Website checkbox](cyberduck-enable-website.png)
